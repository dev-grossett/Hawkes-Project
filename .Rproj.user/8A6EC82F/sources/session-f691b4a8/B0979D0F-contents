---
title: "STAT480 Assignment 3 - Simulation Study"
author: "Devon Grossett"
format: pdf
number-sections: true
bibliography: BIBTEX_SIM.bib
---

```{r source, include=FALSE}
library(dplyr)
library(ggplot2)
source("generate_results.R")

# Helper function to plot nice histograms (there will be a few)
hist_minimal <- function(binwidth, xlab, title = NULL) {
  list(
    geom_histogram(
      binwidth = binwidth,
      fill = "grey30",
      color = "white",
      alpha = 0.75
    ),
    theme_minimal(base_size = 12),
    theme(
      panel.grid.minor = element_blank(),
      panel.grid.major.x = element_blank(),
      panel.grid.major.y = element_line(color = "grey85"),
      axis.title = element_text(size = 11),
      axis.text = element_text(color = "grey20"),
      plot.title = element_text(size = 13, hjust = 0.5),
      plot.title.position = "plot"
    ),
    labs(x = xlab, y = "Count", title = title)
  )
}
```

## Introduction {#sec-intro}

When reporting on a classification model, there are several metrics available to the analyst by which to measure the performance. Each aims to capture a different element of the model's performance, such as: the proportion of correct classifications (accuracy), the proportion of predicted positives that are true positives (precision), or the proportion of true positives/negatives that were correctly classified (recall/specificity). The classification metrics reported on in this study are given in Equations \eqref{eq-accuracy}-\eqref{eq-f1}, where following the convention of the `scikit-learn` Python package [@scikit-learn], undefined Recall and Precision are given values of zero.

\begin{equation}
\text{Accuracy} = \frac{\text{TP} + \text{TN}}{\text{TP} + \text{FP} + \text{TN} + \text{FN}} \label{eq-accuracy}
\end{equation} 

\begin{equation}
\text{Precision} = \frac{\text{TP}}{\text{TP} + \text{FP}} \label{eq-precision}
\end{equation} 

\begin{equation}
\text{Recall} = \frac{\text{TP}}{\text{TP} + \text{FN}} \label{eq-recall}
\end{equation} 

\begin{equation}
\text{Specificity} = \frac{\text{TN}}{\text{TN} + \text{FP}} \label{eq-specificity}
\end{equation}

\begin{equation}
F_1 = \frac{2 \cdot \text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}} = \frac{2\text{TP}}{2\text{TP} + \text{FP} + \text{FN}}  \label{eq-f1}
\end{equation} 


This report studies the empirical distributions and relationships between the above classification metrics on a simulated dataset with a high class imbalance (5% positive class). For each Monte Carlo (MC) trial, the model is trained on a relatively small training set of 30 observations, and scored on validation set of 120 observations. For each MC trial, training and evaluation of a bayesian linear discriminant classifier on this simulated data allows us to examine the performance of, and dependence between metrics. 

Important findings are the low variability and relatively high performance of the accuracy metric in the presence of large class imbalance, even when problems with the classifier are identified with through the performance of the other metrics, and the trade off evident between the competing precision and recall metrics.

## Methodology {#sec-methods}

### Data Generating Process {#sec-dgp}

For this study, the data consists of two classes we wish to build a classification model to make predictions for. Class label "A" is produced with probability $p_A = 0.95$, and class label "B" is produced with probability $p_B = 1 - p_A = 0.05$. The independent observations $X_1, X_2, \ldots, X_n$, $n=150$, are produced from two normal distributions depending on class membership: $\mathcal{N}_A(-1, 1)$ and $\mathcal{N}_B(1, 1)$. The code to implement this is given in @lst-dgp.

Four of the MC trials were randomly sampled, with the distribution of $X$ values, and overlays of the (unscaled) theoretical densities shown in @fig-4histos. Due to the low probability of being from class "B", the histogram is dominated by the class "A" observations and resembles a slightly right-skewed $\mathcal{N}_A(-1, 1)$ density with a low number of outliers in the right tail that arise from the $\mathcal{N}_B(1, 1)$ density.

```{r, echo=FALSE}
#| label: fig-4histos
#| fig-cap: "Four randomly sampled histograms of our observed X value, with theoretical densities for Class A (red) and B (blue) overlaid"

set.seed(3141)
sampled_plots <- sample(results$mc_trial, 4)
data |> 
  filter(mc_trial %in% sampled_plots) |>
  ggplot(aes(x)) + 
  geom_histogram(
    aes(y = after_stat(density)),
    binwidth = 0.2,
    fill = "grey30",
    color = "white",
    alpha = 0.75
  ) +
  stat_function(
    fun = dnorm,
    args = list(mean = -1, sd = 1),
    colour = "red",
    linetype = "dashed",
    linewidth = 1
  ) +
  stat_function(
    fun = dnorm,
    args = list(mean = 1, sd = 1),
    colour = "blue",
    linetype = "dashed",
    linewidth = 1
  ) +
  theme_minimal(base_size = 12) + 
  theme(
    panel.grid.minor = element_blank(),
    panel.grid.major.x = element_blank(),
    panel.grid.major.y = element_line(color = "grey85"),
    axis.title = element_text(size = 11),
    axis.text = element_text(color = "grey20"),
    plot.title = element_text(size = 13, hjust = 0.5)
  ) +
  labs(x = "X", y = "Density", title = "Monte Carlo Trial") +
  facet_wrap(~ mc_trial)
```

Of the 150 observations, 30 will be randomly selected to train the classification model, and the remaining 120 will be used for calculating the chose classification performance metrics. In order to be able to train the classifier, there must be at least one instance of each class in the training set. For this reason, we discard any MC trials for which there are only one class sampled. While statistically impossible to have only 150 class "B" observations (probability of $7 \times 10^{-196}$), only having 150 class "A" observations has a probability of $\approx 4.6 \times 10^{-4}$ which across our 10,000 planned MC trials we would expect to occur on average 5 times. 

One approach to ensure there is at least one "B" in each training set would be to repeatedly resample until that were the case, however that could require multiple rounds of resampling and be inefficient. Instead, the approach taken was a two-stage sampling process, where we first sample one observation from the available "B" observations in each group and then sample 29 of the remaining 149 observations without replacement. The one forced "B" and the remaining 29 sampled observations form our training set. The code to accomplish this is given in @lst-training. 

### Classification Model {#sec-class_model}

Given that our data generating process is the same as the assumptions that underpin Linear Discriminant Analysis (LDA), that is:

* $f_k(x) \equiv \text{Pr}(X=x | Y=k) \sim \mathcal{N}(\mu_k, \sigma^2_k)$
* $\sigma^2_1 = \sigma^2_2 = \ldots = \sigma^2_k$

we will train this form of a Bayesian classifier to predict the class membership of our hold-out dataset (see @james2013, Section 4.4.1). At its heart, a Bayesian classifier assigns observations to the class with the greatest posterior probability given by Bayes' Theorem in Equation \eqref{eq-bayes}.

\begin{equation}
\text{Pr}(Y=k | X=x) = \frac{\pi_k f_k(x)}{\sum_{l=1}^K \pi_l f_l(x)} \equiv p_k(x) \label{eq-bayes}
\end{equation}

For the LDA classifier, this is further simplified by the assumption of a shared variance term, $\sigma^2$, and that $f_k(x) \sim \mathcal{N}(\mu_k, \sigma^2_k)$. By plugging in the equation for the Normal PDF into \eqref{eq-bayes}, taking logarithms, and removing constant terms, one can show that the class for which \eqref{eq-bayes} is higher is equivalent to finding the class for which Equation \eqref{eq-lda} is greater.

\begin{equation}
\delta_k(x) = x \cdot \frac{\mu_k}{\sigma^2} - \frac{\mu_k^2}{2 \sigma^2} + \log \pi_k \label{eq-lda}
\end{equation}

All that is left is to estimate the values $\mu_k$ and $\pi_k$ for each class, and the shared variance term, $\sigma^2$. While in our case we have simualated the data so know their true values, that is virtually never the case in the real world so for an LDA classifier these mean and variance terms are estimated from the training set using the estimators in Equations \eqref{eq-mu_est} and \eqref{eq-sigma_est}. $\pi_k$ is simply estimated using the proportion of the training observations that belong to the $k$-th class. This gives the linear discriminant used for classification in Equation \eqref{eq-lda_est}. 

\begin{align}
\hat{\mu}_k &= \frac{1}{n_k} \sum_{i:y_i=k} x_i \label{eq-mu_est} \\
\hat{\sigma}^2 &= \frac{1}{n - K} \sum_{k = 1}^K \sum_{i:y_i=k} (x_i - \hat{\mu}_k)^2  \label{eq-sigma_est} \\
\hat{\pi}_k &= \frac{n_k}{n} \label{eq-pi_est} \\
\hat{\delta}_k(x) &= x \cdot \frac{\hat{\mu}_k}{\hat{\sigma}^2} - \frac{\hat{\mu}_k^2}{2 \hat{\sigma}^2} + \log \hat{\pi}_k \label{eq-lda_est}
\end{align} 

## Results {#sec-results}


```{r, echo=FALSE}
#| label: fig-histos
#| fig-cap: "Empirical Distribution of Classification Metrics across Monte Carlo Trials"

ggplot(results_long, aes(value)) +
  geom_histogram(
    binwidth = 0.025,
    fill = "grey30",
    color = "white",
    alpha = 0.75
  ) +
  theme_minimal() + 
  theme(
    panel.grid.minor = element_blank(),
    panel.grid.major.x = element_line(color = "grey85"),
    panel.grid.major.y = element_line(color = "grey85"),
    axis.title = element_text(size = 11),
    axis.text = element_text(color = "grey20"),
    plot.title = element_text(size = 13, hjust = 0.5)
  ) +
  labs(y = "Count", title = "Classification Metrics") +
  facet_wrap(~ Metric, scales = "free_y", ncol = 2) 

```

```{r, echo=FALSE}
#| label: tbl-metrics
#| tbl-cap: "Summary statistics of test classifier metrics."

library(knitr)

summary_table <- results_long |>
  group_by(Metric) |>
  summarise(
    Mean   = mean(value),
    SD     = sd(value),
    Min    = min(value),
    P25    = quantile(value, 0.25),
    Median = median(value),
    P75    = quantile(value, 0.75),
    Max    = max(value),
    .groups = "drop"
  )


kable(summary_table, digits = 3)
```

The distribution of classification metrics MC trials is given in @fig-histos. To emphasise distributional shape over raw counts, histograms were faceted with free y-axis scaling and fixed x-axis scaling. A table of summary statistics is given in @tbl-metrics.


```{r, echo=FALSE}
#| label: fig-pairs
#| fig-cap: "Scatterplot Matrix of Training Classification Metrics"

pairs(results[, 6:10])

```

For exploring the relationship between the different classification metrics, a scatter plot matrix is given in @fig-pairs. Two key relationships are analysed in @sec-discussion, Accuracy-Recall and Precision-Recall. These specific plots are shown in @fig-accuracy_recall and @fig-precision_recall.

```{r, echo=FALSE, message=FALSE}
#| label: fig-accuracy_recall
#| fig-cap: "Scatter plot of Accuracy versus Recall across MC trials. Accuracy remains high even when recall is poor, indicating that overall accuracy is dominated the performance of the majority class when significant class imbalance exists."
#| fig-width: 6.5
#| fig-height: 3

ggplot(results, aes(x = recall, y = accuracy)) +
  geom_point(alpha = 0.4, size = 1.4, colour = "black") +
  geom_smooth(method = "loess", se = FALSE, linewidth = 0.9) +
  coord_cartesian(ylim = c(0.8, 1.0)) +
  theme_minimal(base_size = 12) +
  theme(panel.grid.minor = element_blank()) +
  labs(x = "Recall", y = "Accuracy")
```

```{r, echo=FALSE, message=FALSE}
#| label: fig-precision_recall
#| fig-cap: "Scatter plot of Precision versus Recall across MC trials, highlighting the trade-off between minimising false positives and detecting minority class membership."
#| fig-width: 6.5
#| fig-height: 3
ggplot(results, aes(x = recall, y = precision)) +
  geom_point(alpha = 0.4, size = 1.4, colour = "black") +
  geom_smooth(method = "loess", se = FALSE, linewidth = 0.9) +
  theme_minimal(base_size = 12) +
  theme(panel.grid.minor = element_blank()) +
  labs(x = "Recall", y = "Precision")
```

## Discussion {#sec-discussion}

### Accuracy vs Recall

@fig-accuracy_recall shows an important feature of the data and the classification model. Due to the low rate of minority class observations, the model's accuracy remains high even when the recall of the minority class is poor. @fig-4histos further demonstrates that while accuracy is tightly grouped around the mean of `r round(summary_table$Mean[1], 2)`, while recall is widely spread around its mean of `r round(summary_table$Mean[4], 2)` with a large point mass at zero due to the `scikit-learn` convention of assigning an undefined recall a value of zero. This highlights the importance of utilising classification metrics such as recall when there is large class imbalance, as accuracy can be a misleading metric and optimising for it will tend to produce models that favour predictions of the majority class.

On the contrary, we observe a clear (but non-linear) relationship between precision and recall as seen in @fig-precision_recall. Beyond the point where $\text{recall} \approx 0.25$, as recall increases, precision decreases. This is expected, as precision penalises false positives while recall penalises false negatives.  

The $F_1$ score acts as a balance between precision and recall (being its geometric mean), having higher values only when both precision and recall have high values. A catastrophic value for one of those components will cause the $F_1$ value to perform similarly poorly. In the absence of a clear reason to favour precision or recall (such as False Positives/Negatives being highly undesirable), the $F_1$ score provides a nice middle ground.

Overall, the results demonstrates the interdependence between the classification metrics studies, in particular the trade-off made between precision and recall, and how accuracy can be misleading when read alone in the presence of class imbalance. These findings emphasise show the importance of selecting a classification performance metric that suits the requirements of the analyst, particularly when class imbalance is present.

## References {.unnumbered}

::: {#refs}
:::

\newpage

\setcounter{section}{0}
\renewcommand{\thesection}{\Alph{section}}

\setcounter{table}{0}
\renewcommand{\thetable}{A\arabic{table}}

\setcounter{figure}{0}

## Code Blocks {.appendix}

```{r, eval=FALSE}
#| lst-label: lst-dgp
#| lst-cap: Function to generate Monte Carlo trials

generate_mc_samples <- function(
    p_A,          # Probability of being from case "A"
    mu_A,         # Mean of case "A"
    mu_B,         # Mean of case "B"
    n,            # Observations per trial
    mc_trials,    # Number of Monte Carlo trials
    seed=NULL,    # Random seed (optional)
    discard=TRUE  # Discard MC trials with only one class generated
) {
  # Generates a number of Monte Carlo trials from a system that produces 
  # independent observations X1,X2,... from two different normal distributions.
  # Both distributions have variance=1, but case "A" has mean mu_A, and case "B"
  # has mean mu_B. The system chooses to produce an observation from case "A" 
  # with probability p_A, and from case "B" with probability (1 - p_A)
  
  require(dplyr)
  if (!is.null(seed)) {set.seed(seed)}
  
  # Data frame with mc_trials number of Monte Carlo trials of X_1, X_2, ..., X_n
  # Generated from U(0, 1), where if U(0, 1) <= p_A then the observation belongs
  # to class "A", otherwise it belongs to class "B"
  data <- data.frame(uniform = runif(n*mc_trials)) |> 
    mutate(
      rownum = row_number(),
      mc_trial = (row_number() - 1) %/% n + 1,
      group = if_else(uniform <= p_A, "A", "B"),
      # mean value behind data generating process
      mean = if_else(uniform <= p_A, mu_A, mu_B)
    ) |> 
    # Discard our U(0, 1) variable
    select(-uniform)

  # Add in random normal variables from class means with variance 1
  data$x <- rnorm(n*mc_trials, data$mean)
  
  # Remove MC Trials that only contain one class
  if (discard) {
    data <- data |> 
      group_by(mc_trial) |> 
      filter(any(group == "A") & any(group == "B")) |> 
      ungroup()
  }
  return(data)
}
```


```{r, eval=FALSE}
#| lst-label: lst-training
#| lst-cap: Assigning training observations for each Monte Carlo trial
# Assign 30 observations from each MC trial to be in the training group
data <- data |>
  group_by(mc_trial) |>
  mutate(
    train = {
      idx <- seq_len(n())
      B_idx <- idx[group == "B"]

      forced_B <- B_idx[sample.int(length(B_idx), 1)]
      rest <- sample(setdiff(idx, forced_B), 29)

      idx %in% c(forced_B, rest)
    }
  ) |>
  ungroup()
```
